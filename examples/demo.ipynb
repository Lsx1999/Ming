{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ Setup & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoProcessor, GenerationConfig\n",
    "from modeling_bailingmm import BailingMMNativeForConditionalGeneration\n",
    "\n",
    "# Load pre-trained model with optimized settings\n",
    "model = BailingMMNativeForConditionalGeneration.from_pretrained(\n",
    "    \"inclusionAI/Ming-Lite-Omni\",\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    "    low_cpu_mem_usage=True       # Minimize CPU memory during loading\n",
    ").to(\"cuda\")                     # Run on GPU\n",
    "\n",
    "# Initialize processor for handling multimodal inputs\n",
    "assets_path = YOUR_ASSETS_PATH   # Set your media directory path\n",
    "processor = AutoProcessor.from_pretrained(\"inclusionAI/Ming-Lite-Omni\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💬 Text QA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"请详细介绍鹦鹉的生活习性。\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# 鹦鹉是一种非常聪明和社交性强的鸟类，它们的生活习性非常丰富和有趣。以下是一些关于鹦鹉生活习性的详细介绍：\n",
    "# ### 1. **栖息地**\n",
    "# 鹦鹉主要分布在热带和亚热带地区，包括非洲、亚洲、澳大利亚和南美洲。它们通常生活在森林、草原、沙漠和城市环境中。不同种类的鹦鹉对栖息地的要求有所不同，但大多数鹦鹉喜欢有丰富植被和水源的地方。\n",
    "# ### 2. **饮食**\n",
    "# 鹦鹉是杂食性动物，它们的饮食非常多样化。它们的食物包括种子、坚果、水果、蔬菜、花蜜和昆虫。鹦鹉的喙非常强壮，能够轻松地打开坚硬的果壳和坚果。一些鹦鹉还会吃泥土或沙子，以帮助消化和补充矿物质。\n",
    "# ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🖼️ Image QA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image qa\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": os.path.join(assets_path, \"flowers.jpg\")},\n",
    "            {\"type\": \"text\", \"text\": \"What kind of flower is this?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# The flowers in this image are forget-me-nots. These delicate blooms are known for their small, five-petaled flowers that come in various shades of blue, pink, and white. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Chain-of-Thought Reasoning\n",
    "**To enable thinking before response, adding the following system prompt before your question:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = \"SYSTEM: You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in <thinking>...</thinking> tags, then the final answer enclosed in <answer>...</answer> tags. The critical answer or key result should be placed within \\\\boxed{}.\\n\"\n",
    "# And your input message should be like this:\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": os.path.join(assets_path, \"reasoning.png\")},\n",
    "            {\"type\": \"text\", \"text\": cot_prompt + \"In the rectangle $A B C D$ pictured, $M_{1}$ is the midpoint of $D C, M_{2}$ the midpoint of $A M_{1}, M_{3}$ the midpoint of $B M_{2}$ and $M_{4}$ the midpoint of $C M_{3}$. Determine the ratio of the area of the quadrilateral $M_{1} M_{2} M_{3} M_{4}$ to the area of the rectangle $A B C D$.\\nChoices:\\n(A) $\\frac{7}{16}$\\n(B) $\\frac{3}{16}$\\n(C) $\\frac{7}{32}$\\n(D) $\\frac{9}{32}$\\n(E) $\\frac{1}{5}$\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "# \\<think\\>\\nOkay, so I have this problem about a rectangle ABCD ... (thinking process omitted) ... So, the correct answer is C.\\n\\</think\\>\\n\\<answer\\>\\\\boxed{C}\\</answer\\>\\n\\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎥 Video QA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video qa\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": os.path.join(assets_path, \"yoga.mp4\")},\n",
    "            {\"type\": \"text\", \"text\": \"What is the woman doing?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# The image shows a woman performing a yoga pose on a rooftop. She's in a dynamic yoga pose, with her arms and legs extended in various positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💬 Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-turn chat\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"中国的首都是哪里？\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"ASSISTANT\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"北京\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"它的占地面积是多少？有多少常住人口？\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# 北京市的总面积约为16,410.54平方公里，常住人口约为21,542,000人。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Format inputs using chat template\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "# 2. Extract vision/audio data\n",
    "image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)\n",
    "\n",
    "# 3. Prepare tensor inputs\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    audios=audio_inputs,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "for k in inputs.keys():\n",
    "    if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "        inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "\n",
    "# 4. Configure generation\n",
    "generation_config = GenerationConfig.from_dict({'no_repeat_ngram_size': 10})\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    eos_token_id=processor.gen_terminator,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "# 5. Decode output\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔊 Audio Tasks\n",
    "\n",
    "For detailed usage for ASR, SpeechQA, and TTS tasks, please refer to `test_audio_tasks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\"},\n",
    "            {\"type\": \"audio\", \"audio\": 'data/wavs/BAC009S0915W0292.wav'},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# we use whisper encoder for ASR task, so need modify code above\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    audios=audio_inputs,\n",
    "    return_tensors=\"pt\",\n",
    "    audio_kwargs={'use_whisper_encoder': True}\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    eos_token_id=processor.gen_terminator,\n",
    "    generation_config=generation_config,\n",
    "    use_whisper_encoder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech2speech\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": 'data/wavs/speechQA_sample.wav'},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "generation_config = GenerationConfig.from_dict({\n",
    "    'output_hidden_states': True,\n",
    "    'return_dict_in_generate': True,\n",
    "    'no_repeat_ngram_size': 10}\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    eos_token_id=processor.gen_terminator,\n",
    "    generation_config=generation_config,\n",
    "    use_whisper_encoder=False\n",
    ")\n",
    "\n",
    "generated_ids = outputs.sequences\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# speechQA result\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "# for TTS\n",
    "from modeling_bailing_talker import AudioDetokenizer\n",
    "\n",
    "model_name_or_path = model.config._name_or_path\n",
    "audio_detokenizer = AudioDetokenizer(\n",
    "    f'{model_name_or_path}/talker/audio_detokenizer.yaml',\n",
    "    flow_model_path=f'{model_name_or_path}/talker/flow.pt',\n",
    "    hifigan_model_path=f'{model_name_or_path}/talker/hift.pt'\n",
    ")\n",
    "spk_input = torch.load('data/spks/luna.pt')\n",
    "thinker_reply_part = outputs.hidden_states[0][0] + outputs.hidden_states[0][-1]\n",
    "# Setting thinker_reply_part to None allows the talker to operate as a standalone TTS model, independent of the language model.\n",
    "audio_tokens = model.talker.omni_audio_generation(\n",
    "    output_text, \n",
    "    thinker_reply_part=thinker_reply_part, **spk_input)\n",
    "waveform = audio_detokenizer.token2wav(audio_tokens, save_path='out.wav', **spk_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎨 Image Generation & Editing\n",
    "\n",
    "Ming-omni natively supports image generation and image editing. To use this function, you only need to add the corresponding parameters in the generate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generation mode currently limits the range of input pixels.\n",
    "gen_input_pixels = 451584\n",
    "processor.max_pixels = gen_input_pixels\n",
    "processor.min_pixels = gen_input_pixels\n",
    "\n",
    "def generate(messages, processor, model, **image_gen_param):\n",
    "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        audios=audio_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    for k in inputs.keys():\n",
    "        if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "            inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "    \n",
    "    print(image_gen_param)\n",
    "    image = model.generate(\n",
    "        **inputs,\n",
    "        image_gen=True,\n",
    "        **image_gen_param,\n",
    "    )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-to-image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Draw a girl with short hair.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "image = generate(\n",
    "   messages=messages, processor=processor, model=model, \n",
    "   image_gen_cfg=6.0, image_gen_steps=20, image_gen_width=480, image_gen_height=544\n",
    ")\n",
    "image.save(\"./t2i.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"samples/cake.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"add a candle on top of the cake\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "image = generate(\n",
    "   messages=messages, processor=processor, model=model, \n",
    "   image_gen_cfg=6.0, image_gen_steps=20, image_gen_width=512, image_gen_height=512\n",
    ")\n",
    "image.save(\"./edit.jpg\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
